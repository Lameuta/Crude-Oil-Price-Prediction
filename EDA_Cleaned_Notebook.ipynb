{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d390e80",
      "metadata": {
        "id": "8d390e80"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "This notebook combines all features (old and new), explores them, visualizes distributions and correlations, and keeps only relevant features based on correlation with the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ad3464",
      "metadata": {
        "id": "67ad3464"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296fc3a7",
      "metadata": {
        "id": "296fc3a7"
      },
      "source": [
        "## 1. Load Combined Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a47d64b",
      "metadata": {
        "id": "2a47d64b"
      },
      "outputs": [],
      "source": [
        "TICKERS = {\n",
        "    # === CORE OIL & ENERGY ===\n",
        "    'WTI_Crude': 'CL=F',           # Target variable\n",
        "    'Brent_Oil': 'BZ=F',           # Alternative oil benchmark\n",
        "    'Natural_Gas': 'NG=F',         # Natural gas futures\n",
        "    'RBOB_Gasoline': 'RB=F',       # Gasoline futures\n",
        "    'Heating_Oil': 'HO=F',         # Heating oil futures\n",
        "\n",
        "    # === EQUITY INDICES ===\n",
        "    'SP500': '^GSPC',              # S&P 500\n",
        "    'NASDAQ': '^IXIC',             # NASDAQ\n",
        "    'Russell2000': '^RUT',         # Small cap\n",
        "    'FTSE100': '^FTSE',            # UK market\n",
        "    'DAX': '^GDAXI',               # German market\n",
        "    'Nikkei': '^N225',             # Japanese market\n",
        "\n",
        "    # === CURRENCIES ===\n",
        "    'DXY': 'DX-Y.NYB',             # US Dollar Index\n",
        "    'EUR_USD': 'EURUSD=X',         # Euro/USD\n",
        "    'GBP_USD': 'GBPUSD=X',         # British Pound/USD\n",
        "    'CAD_USD': 'CADUSD=X',         # Canadian Dollar/USD (oil exporter)\n",
        "    'NOK_USD': 'NOKUSD=X',         # Norwegian Krone/USD (oil exporter)\n",
        "\n",
        "    # === INTEREST RATES & BONDS ===\n",
        "    'US2Y': '^IRX',                # 2-year Treasury\n",
        "    'US5Y': '^FVX',                # 5-year Treasury\n",
        "    'US10Y': '^TNX',               # 10-year Treasury\n",
        "    'US30Y': '^TYX',               # 30-year Treasury\n",
        "\n",
        "    # === COMMODITIES ===\n",
        "    'Gold': 'GC=F',                # Gold futures\n",
        "    'Silver': 'SI=F',              # Silver futures\n",
        "    'Copper': 'HG=F',              # Copper futures\n",
        "\n",
        "    # === VOLATILITY & RISK ===\n",
        "    'VIX': '^VIX',                 # Volatility Index\n",
        "\n",
        "    # === SECTOR ETFS ===\n",
        "    'Energy_ETF': 'XLE',           # Energy sector ETF\n",
        "    'Oil_ETF': 'USO',              # Oil ETF\n",
        "    'Transportation': 'IYT',        # Transportation ETF\n",
        "    'Materials': 'XLB',            # Materials sector\n",
        "    'Emerging_Markets': 'EEM'       # Emerging markets\n",
        "}\n",
        "\n",
        "# Date configuration\n",
        "START_DATE = '2000-01-01'\n",
        "END_DATE = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "\n",
        "print(f\"ðŸ“Š Configured {len(TICKERS)} market indicators\")\n",
        "print(f\"ðŸ“… Date range: {START_DATE} to {END_DATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae890349",
      "metadata": {
        "id": "ae890349"
      },
      "source": [
        "## 2. Basic Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fec5e7",
      "metadata": {
        "id": "99fec5e7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "def download_market_data(tickers_dict, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Download Close price market data for multiple tickers using yfinance.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tickers_dict : dict\n",
        "        Dictionary of {name: ticker_symbol}\n",
        "    start_date : str\n",
        "        Start date in 'YYYY-MM-DD' format\n",
        "    end_date : str\n",
        "        End date in 'YYYY-MM-DD' format\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame : Close prices with renamed columns\n",
        "    dict : Failed downloads with error messages\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ“ˆ Downloading market data...\")\n",
        "\n",
        "    # Validate date format\n",
        "    try:\n",
        "        datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "        datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Dates must be in 'YYYY-MM-DD' format.\")\n",
        "\n",
        "    # Extract tickers and reverse-lookup for name remapping\n",
        "    ticker_list = list(tickers_dict.values())\n",
        "    reverse_map = {v: k for k, v in tickers_dict.items()}\n",
        "\n",
        "    try:\n",
        "        # Batch download all tickers at once\n",
        "        data = yf.download(ticker_list, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "        # Handle single vs multi-level columns\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            close_prices = data['Close']\n",
        "        else:\n",
        "            close_prices = data.dropna(how='all')  # fallback if only 1 ticker\n",
        "\n",
        "        # Drop completely empty columns\n",
        "        close_prices = close_prices.dropna(axis=1, how='all')\n",
        "\n",
        "        # Rename columns to original names\n",
        "        close_prices.rename(columns=reverse_map, inplace=True)\n",
        "\n",
        "        # Filter out tickers with insufficient data\n",
        "        failed_downloads = {}\n",
        "        for name in tickers_dict.keys():\n",
        "            if name not in close_prices.columns:\n",
        "                failed_downloads[name] = \"No data downloaded\"\n",
        "            elif close_prices[name].dropna().shape[0] < 100:\n",
        "                failed_downloads[name] = f\"Insufficient data ({close_prices[name].dropna().shape[0]} points)\"\n",
        "                close_prices.drop(columns=name, inplace=True)\n",
        "\n",
        "        if not close_prices.empty:\n",
        "            print(f\"âœ… Successfully downloaded {close_prices.shape[1]} out of {len(tickers_dict)} tickers.\")\n",
        "        else:\n",
        "            print(\"âŒ No valid data found.\")\n",
        "\n",
        "        # Log failures\n",
        "        if failed_downloads:\n",
        "            print(f\"\\nâš ï¸  Failed downloads: {len(failed_downloads)}\")\n",
        "            for name, reason in failed_downloads.items():\n",
        "                print(f\"   â€¢ {name}: {reason}\")\n",
        "\n",
        "        return close_prices if not close_prices.empty else None, failed_downloads\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error downloading data: {e}\")\n",
        "        return None, {name: \"Batch download failed\" for name in tickers_dict}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j11dFGRaJEVo",
      "metadata": {
        "id": "j11dFGRaJEVo"
      },
      "outputs": [],
      "source": [
        "market_data, failed_downloads = download_market_data(TICKERS, START_DATE, END_DATE)\n",
        "\n",
        "if market_data is None:\n",
        "  print(\"âŒ Failed to download any market data. Exiting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpf6-bzgKzpY",
      "metadata": {
        "id": "rpf6-bzgKzpY"
      },
      "outputs": [],
      "source": [
        "market_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e4885af",
      "metadata": {
        "id": "4e4885af"
      },
      "source": [
        "## 3. Missing Value Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7eb85c1",
      "metadata": {
        "id": "c7eb85c1"
      },
      "outputs": [],
      "source": [
        "def clean_and_process_data(market_data, add_sentiment=True):\n",
        "    \"\"\"\n",
        "    Clean market data and add engineered features\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    market_data : pd.DataFrame\n",
        "        Raw market data\n",
        "    add_sentiment : bool\n",
        "        Whether to add sentiment features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame : Cleaned and processed data\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ§¹ Cleaning and processing data...\")\n",
        "    print(f\"   Initial shape: {market_data.shape}\")\n",
        "\n",
        "    # Create copy for processing\n",
        "    df = market_data.copy()\n",
        "\n",
        "    # Handle missing values\n",
        "    print(f\"   Missing values before cleaning:\")\n",
        "    missing_before = df.isnull().sum()\n",
        "    for col, missing in missing_before[missing_before > 0].items():\n",
        "        print(f\"     â€¢ {col}: {missing:,} ({missing/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Forward fill missing values (common for financial data)\n",
        "    df = df.fillna(method='ffill')\n",
        "\n",
        "    # Backward fill any remaining NaN at the beginning\n",
        "    df = df.fillna(method='bfill')\n",
        "\n",
        "    # Drop any remaining NaN rows\n",
        "    rows_before = len(df)\n",
        "    df = df.dropna()\n",
        "    rows_after = len(df)\n",
        "\n",
        "    if rows_before != rows_after:\n",
        "        print(f\"   Removed {rows_before - rows_after:,} rows with NaN values\")\n",
        "\n",
        "    # Add sentiment features if requested\n",
        "    if add_sentiment:\n",
        "        print(f\"   Adding sentiment features...\")\n",
        "\n",
        "        # Generate sentiment data\n",
        "        sentiment_series = generate_sentiment_data(df.index)\n",
        "        df['News_Sentiment'] = sentiment_series\n",
        "\n",
        "        # Create sentiment-based features\n",
        "        df['Sentiment_7d_MA'] = df['News_Sentiment'].rolling(window=7, min_periods=1).mean()\n",
        "        df['Sentiment_30d_MA'] = df['News_Sentiment'].rolling(window=30, min_periods=1).mean()\n",
        "        df['Sentiment_Volatility'] = df['News_Sentiment'].rolling(window=30, min_periods=1).std()\n",
        "\n",
        "        print(f\"   âœ… Added 4 sentiment-based features\")\n",
        "\n",
        "    print(f\"   Final shape: {df.shape}\")\n",
        "    print(f\"   Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a5193c",
      "metadata": {
        "id": "20a5193c"
      },
      "outputs": [],
      "source": [
        "def generate_sentiment_data(date_index, seed=42):\n",
        "    \"\"\"\n",
        "    Generate simulated sentiment data for demonstration\n",
        "\n",
        "    In production, replace this with real news sentiment analysis:\n",
        "    - News API integration\n",
        "    - NLP sentiment analysis\n",
        "    - Social media sentiment\n",
        "    - Economic reports sentiment\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    date_index : pd.DatetimeIndex\n",
        "        Date index to match market data\n",
        "    seed : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.Series : Sentiment scores indexed by date\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ“° Generating simulated sentiment data...\")\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    n_days = len(date_index)\n",
        "\n",
        "    # Create more realistic sentiment with autocorrelation\n",
        "    sentiment_scores = np.zeros(n_days)\n",
        "    sentiment_scores[0] = np.random.normal(0, 0.3)\n",
        "\n",
        "    # AR(1) process for persistence\n",
        "    for i in range(1, n_days):\n",
        "        sentiment_scores[i] = (0.7 * sentiment_scores[i-1] +\n",
        "                              0.3 * np.random.normal(0, 0.3))\n",
        "\n",
        "    # Add occasional extreme events (simulate major news)\n",
        "    extreme_events = np.random.choice(n_days, size=int(n_days * 0.01), replace=False)\n",
        "    sentiment_scores[extreme_events] += np.random.choice([-1.2, 1.2], size=len(extreme_events))\n",
        "\n",
        "    # Clip to reasonable range\n",
        "    sentiment_scores = np.clip(sentiment_scores, -2, 2)\n",
        "\n",
        "    sentiment_series = pd.Series(sentiment_scores, index=date_index, name='News_Sentiment')\n",
        "\n",
        "    print(f\"âœ… Generated sentiment data with:\")\n",
        "    print(f\"   â€¢ Mean: {sentiment_series.mean():.4f}\")\n",
        "    print(f\"   â€¢ Std:  {sentiment_series.std():.4f}\")\n",
        "    print(f\"   â€¢ Min:  {sentiment_series.min():.4f}\")\n",
        "    print(f\"   â€¢ Max:  {sentiment_series.max():.4f}\")\n",
        "\n",
        "    return sentiment_series\n",
        "\n",
        "def analyze_text_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using TextBlob (for future real implementation)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        Text to analyze\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Sentiment polarity score (-1 to 1)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        return blob.sentiment.polarity\n",
        "    except:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aIEt3s2VLTUs",
      "metadata": {
        "id": "aIEt3s2VLTUs"
      },
      "outputs": [],
      "source": [
        "processed_data = clean_and_process_data(market_data, add_sentiment=True)\n",
        "processed_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5335c5",
      "metadata": {
        "id": "9f5335c5"
      },
      "source": [
        "## 4. Feature Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda63a06",
      "metadata": {
        "id": "cda63a06"
      },
      "source": [
        "## 5. Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057cdf08",
      "metadata": {
        "id": "057cdf08"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def analyze_feature_relationships_with_oil(data, target_col='WTI_Crude', threshold=0.3, top_n=10):\n",
        "    \"\"\"\n",
        "    Perform feature correlation and importance analysis with respect to oil prices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        Market data (features as columns).\n",
        "    target_col : str\n",
        "        Name of the target feature (default: 'WTI_Crude').\n",
        "    threshold : float\n",
        "        Absolute correlation threshold to select relevant features.\n",
        "    top_n : int\n",
        "        Number of top correlated features to display/analyze.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    selected_features : list\n",
        "        List of selected feature names (including target).\n",
        "    correlations : pd.Series\n",
        "        Series of feature correlations with target_col.\n",
        "    \"\"\"\n",
        "    if target_col not in data.columns:\n",
        "        print(f\"âŒ Target column '{target_col}' not found in data.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Analyzing feature relationships with {target_col}...\")\n",
        "\n",
        "    # 1. Correlation Matrix\n",
        "    corr_matrix = data.corr()\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)\n",
        "    plt.title(\"ðŸ“Œ Feature Correlation Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation with Target\n",
        "    correlations = corr_matrix[target_col].drop(target_col)\n",
        "    correlations_abs = correlations.abs().sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Top {top_n} Correlations with {target_col}:\")\n",
        "    for i, (feature, abs_corr) in enumerate(correlations_abs.head(top_n).items(), 1):\n",
        "        actual_corr = correlations[feature]\n",
        "        print(f\"   {i:2}. {feature:25}: {actual_corr:+.3f} (|{abs_corr:.3f}|)\")\n",
        "\n",
        "    # 3. Bar Chart of Top-N Correlations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    top_corr = correlations_abs.head(top_n)\n",
        "    sns.barplot(x=top_corr.values, y=top_corr.index, orient='h', palette='coolwarm')\n",
        "    plt.title(f\"ðŸ“Š Top {top_n} Absolute Correlations with {target_col}\")\n",
        "    plt.xlabel(\"Correlation (absolute)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Feature Selection\n",
        "    selected_features_corr = correlations_abs[correlations_abs >= threshold]\n",
        "    selected_features = [target_col] + list(selected_features_corr.index)\n",
        "\n",
        "    print(f\"\\nâœ… Selected {len(selected_features)-1} features with |correlation| â‰¥ {threshold}\")\n",
        "\n",
        "    # 5. Pairplot for selected features\n",
        "    if len(selected_features) <= 10:\n",
        "        sns.pairplot(data[selected_features].dropna())\n",
        "        plt.suptitle(\"ðŸ“· Pairwise Relationships (Selected Features)\", y=1.02)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"ðŸ“· Skipping pairplot (too many features selected).\")\n",
        "\n",
        "    # 6. Feature Importance (Random Forest)\n",
        "    print(\"\\nðŸŒ² Calculating feature importances (Random Forest)...\")\n",
        "    feature_data = data[selected_features].dropna()\n",
        "    X = feature_data.drop(columns=target_col)\n",
        "    y = feature_data[target_col]\n",
        "\n",
        "    # Normalize features for importance comparison\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    rf.fit(X_scaled, y)\n",
        "\n",
        "    importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    importances.plot(kind='barh', color='teal')\n",
        "    plt.title(\"ðŸŒŸ Feature Importances (Random Forest)\")\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 7. Histogram/Distribution of Selected Features\n",
        "    print(\"\\nðŸ“ˆ Plotting distributions of selected features...\")\n",
        "    selected_data = data[selected_features].dropna()\n",
        "    selected_data.hist(figsize=(15, 10), bins=30, layout=(len(selected_features) // 3 + 1, 3))\n",
        "    plt.suptitle(\"ðŸ“Š Distributions of Selected Features\", fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "        # 8. Scatterplots vs. Target\n",
        "    print(\"\\nðŸ“Œ Scatterplots of selected features vs. target...\")\n",
        "    max_plots = min(9, len(selected_features) - 1)\n",
        "    fig, axes = plt.subplots(nrows=(max_plots + 2) // 3, ncols=3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, feature in enumerate(selected_features_corr.head(max_plots).index):\n",
        "        ax = axes[i]\n",
        "        sns.scatterplot(data=data, x=feature, y=target_col, ax=ax, alpha=0.5)\n",
        "        corr_val = correlations[feature]\n",
        "        ax.set_title(f\"{feature} (r = {corr_val:.2f})\")\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"ðŸ“‰ Scatterplots vs. {target_col}\", fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return selected_features, correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aqzvzsvLdUJ",
      "metadata": {
        "id": "5aqzvzsvLdUJ"
      },
      "outputs": [],
      "source": [
        "selected_features, correlations = analyze_feature_relationships_with_oil(\n",
        "            processed_data, target_col='WTI_Crude', threshold=0.3\n",
        "        )\n",
        "final_data = processed_data[selected_features].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d4f3cb",
      "metadata": {
        "id": "c0d4f3cb"
      },
      "source": [
        "## 6. Select Highly Correlated Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858d500c",
      "metadata": {
        "id": "858d500c"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"ðŸ“‹ FINAL DATASET SUMMARY\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"   â€¢ Shape: {final_data.shape[0]:,} rows Ã— {final_data.shape[1]:,} columns\")\n",
        "print(f\"   â€¢ Date range: {final_data.index.min().strftime('%Y-%m-%d')} to {final_data.index.max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"   â€¢ Features included: {final_data.shape[1]:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04fb0df",
      "metadata": {
        "id": "b04fb0df"
      },
      "source": [
        "## 7. Save Relevant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d43504e",
      "metadata": {
        "id": "3d43504e"
      },
      "outputs": [],
      "source": [
        "# Save the filtered relevant features\n",
        "final_data.to_csv(\"relevant_features.csv\")\n",
        "print(\"Saved to relevant_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8oc1Ff4KBi3F",
      "metadata": {
        "id": "8oc1Ff4KBi3F"
      },
      "outputs": [],
      "source": [
        "final_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bJlXyzxwBlfl",
      "metadata": {
        "id": "bJlXyzxwBlfl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
